---
title: '`ahead`: Univariate and multivariate time series forecasting with uncertainty quantification (including simulation approaches)'
tags:
- Python
- R
- Julia
- Time series
- Forecasting
- Machine Learning 
- Uncertainty 
date: "March 10, 2024"
output:
  pdf_document: default
  html_document:
    df_print: paged
urlcolor: blue
author: "T. Moudiki"
affiliation: "techtonique.github.io"
authors:
- name: "T. Moudiki"
  orcid: "0000-0002-9422-5459"
  equal-contrib: yes
  affiliation: 1, 2
- name: Author with no affiliation
  corresponding: yes
  affiliation: 3
bibliography: biblio.bib
aas-doi: "10.3847/xxxxx <- update this with the DOI from AAS once you know it."
aas-journal: "Journal of Open Source Software."
affiliations:
- name: T Moudiki
---

# 1 - Introduction and context

Univariate and Multivariate time series (MTS hereafter) are collections of sequential data points observed at different timesteps for measurable indicators. Real-world examples of MTS include -- among many others -- the monthly totals of international airline passengers from 1949 to 1960, or the daily closing prices of major European stock indices from 1991 to 1998. 

Forecasting MTS is important for business planning and decision support in finance, insurance, and other industries such as *Energy* (load anticipation) and meteorology. One can obtain point forecasts from a statistical/Machine Learning model, but these point forecasts are generally of limited importance to analysts. What matters more is the model's ability to quantify the uncertainty around its  predictions. In finance or insurance for example, uncertainty quantification through simulation is crucial for risk management, liabilities' reserving, and capital valuation.

There are multiple MTS forecasting methods implemented in [\textsf{R} package](https://github.com/Techtonique/ahead)  `ahead`'s version `0.11.0` (there are also [\textsf{Python}](https://github.com/Techtonique/ahead_python) and [\textsf{Julia}](https://github.com/Techtonique/Ahead.jl) implementations, both following \textsf{R}'s API as closely as possible). `ahead` is available through the [R-universe](https://techtonique.r-universe.dev/builds), which allows the package to be continuously integrated and distributed across all major platforms (Windows, macOS, Linux). 

All of `ahead`'s forecasting methods include parametric prediction intervals alongside non-parametric, simulation-based uncertainty quantification techniques. This paper describes **two** of these Machine Learning methods, the _original_ ones, **not available in any other statistical software**:

* \mbox{\texttt{dynrmf}}; an autoregressive dynamic Machine Learning model inspired by NNAR (**N**eural **N**etwork **A**uto**r**egression, see @hyndman2013forecasting. As NNAR, \mbox{\texttt{dynrmf}} does an automatic choice of the number of autoregressive and seasonal time series lags. However, instead of an artificial neural network, as implemented in NNAR, \mbox{\texttt{dynrmf}} __can use any regression (Machine Learning) model__. 

* The forecasting model from @moudiki2018multiple: \mbox{\texttt{ahead::ridge2f}}. \mbox{\texttt{ahead::ridge2f}} implements a __quasi-randomized *neural* networks__ model extending [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) with 2 regularization parameters, and capable of producing highly nonlinear outputs through the use of a *hidden layer*. \newline Since its first publication in 2018, \mbox{\texttt{ahead::ridge2f}} has been enhanced for integrating uncertainty quantification through the (independent, circular,  moving block) bootstrap (@efron1986bootstrap) and copulas simulations (@brechmann2013modeling, @nagler2023vine). Future (ongoing)  developments include conformal prediction (@vovk2005algorithmic) and Kernel Density Estimation (@silverman2018density). 


# 2 - Examples of use 

## 2 - 1 How to install `ahead` in \textsf{R}

```{r "0-install-package", echo=TRUE, message=FALSE, warning=FALSE, include=FALSE, cache=TRUE}
options(repos = c(
    techtonique = 'https://techtonique.r-universe.dev',
    CRAN = 'https://cloud.r-project.org'))
suppressWarnings(suppressMessages(utils::install.packages("rmarkdown", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages("remotes", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages("forecast", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages("ggplot2", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages("e1071", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages("randomForest", repos = c(CRAN="https://cloud.r-project.org"))))
suppressWarnings(suppressMessages(utils::install.packages(c("ahead", "dfoptim"))))
```

```R
options(repos = c(
    techtonique = 'https://techtonique.r-universe.dev',
    CRAN = 'https://cloud.r-project.org'))
utils::install.packages("rmarkdown", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages("remotes", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages("forecast", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages("ggplot2", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages("e1071", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages("randomForest", repos = c(CRAN="https://cloud.r-project.org"))
utils::install.packages(c("ahead", "dfoptim"))
```

**Loading packages**

```{r "1-load-library", echo=TRUE, message=FALSE, warning=FALSE, include=TRUE}
suppressPackageStartupMessages(library(ahead))
suppressPackageStartupMessages(library(forecast))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(e1071))
```


## 2 - 2 Use `ahead::ridge2f`

## 2 - 2 - 1 Use `ahead::ridge2f` for univariate time series forecasting

In all these examples, in order to capture the nonlinear patterns of the inputs, 5 nodes in the hidden layer and a ReLU activation function are used (default hyperparameters, see @goodfellow2016deep and @moudiki2018multiple for more details). 

The `fdeaths` data set below contains **monthly deaths of females from bronchitis, emphysema and asthma in the UK, 1974-1979**. The data were collected by the Office of Population Censuses and Surveys. Mortality data are widely used in insurance, actuarial science and demography. They are notably useful for the valuation of death-linked liabilities.

Here's how to obtain 20-steps-ahead forecasts for `fdeaths` with `ahead::ridge2f`; including seasonality terms. The default level for the prediction interval is 95%. 

```{r "1-ridge2-uni", echo=TRUE, include=TRUE, fig.width=6}
x <- fdeaths # input dataset
xreg <- ahead::createtrendseason(x) # add seasonality and trend
z <- ahead::ridge2f(x, xreg = xreg, h=20L) # forecasting h-steps ahead
ggplot2::autoplot(z) # plot forecast
```

`EuStocksLogReturns` contains the daily log-returns of major European stock indices, Germany DAX (Ibis), Switzerland SMI, France CAC, and UK FTSE, with 1860 observations. Only the first 100 dates of DAX index are used in the example below. 

```{r "1-ridge2-uni-2", echo=TRUE, include=TRUE, fig.width=6}
data(EuStockMarkets)
EuStocks <- ts(EuStockMarkets[1:100, ],
               start = start(EuStockMarkets),
               frequency = frequency(EuStockMarkets)) # original data
EuStocksLogReturns <- ahead::getreturns(EuStocks, type = "log") # obtain log-returns
res <- ahead::ridge2f(EuStocksLogReturns[, "DAX"], h = 20L,
                        type_pi = 'movingblockbootstrap',
                        B = 50L,
                        show_progress = FALSE)
ggplot2::autoplot(res) # plot forecast
```

## 2 - 2 - 2 Use `ahead::dynrmf` for univariate time series forecasting

`fdeaths` is used in this example too. The default model used by `ahead::dynrmf` is an automated ridge regression (automatic choice of the regularization parameter using Leave-One-Out cross-validation, see @bergmeir2018note): 

**- Forecasting with `randomForest::randomForest`**

```{r "4-dynrmf-example", echo=TRUE, include=TRUE, fig.width=8, fig.height=4}
# Plotting forecasts
# With a Random Forest regressor, an horizon of 20, 
# and a 95% prediction interval
fit_rf <- dynrmf(fdeaths, h=20, level=95, fit_func = randomForest::randomForest,
      fit_params = list(ntree = 50), predict_func = predict)
ggplot2::autoplot(fit_rf)
```

Check in-sample residuals:

```{r "4-dynrmf-example-resids", echo=TRUE, include=TRUE, fig.width=8, fig.height=4}
forecast::checkresiduals(fit_rf)
```

\newpage

**- Forecasting with `e1071::svm`** (Support Vector Machines)

```{r "5-dynrmf-example", echo=TRUE, include=TRUE, fig.width=8, fig.height=3}
# With a Support Vector Machine regressor, an horizon of 20, 
# and a 95% prediction interval
fit_svm <- dynrmf(fdeaths, h=20, level=95, fit_func = e1071::svm,
fit_params = list(kernel = "linear"), predict_func = predict)
ggplot2::autoplot(fit_svm)
```

Check in-sample residuals:

```{r "5-dynrmf-example-resids", echo=TRUE, include=TRUE, fig.width=8, fig.height=3}
forecast::checkresiduals(fit_svm)
```

\newpage

**- Use of an external regressor** (trend)

`AirPassengers`'s been widely tested in the specialized literature, because it has a trend, 
a seasonality, and is heteroskedastic (non-constant variance). 

```{r}
h <- 20L
res6 <- ahead::dynrmf(AirPassengers, xreg_fit = 1:length(AirPassengers),
                       xreg_predict = (length(AirPassengers)+1):(length(AirPassengers)+h), 
                      h=h)
autoplot(res6)
```


## 2 - 2 - 2 Use `ahead::ridge2f` for multivariate time series forecasting

The `insurance` dataset available in @hyndman2013forecasting contains monthly quotations and monthly television advertising expenditure for a US insurance company. January 2002 to April 2005. A fast calibration of `ahead::ridge2f` uses a remarkable result available for linear models' Leave-One-Out Cross Validation (LOOCV):

$$
LOOCV error = \frac{1}{n} \sum_{i=1}^n\left(y_i-\hat{f}_{-i}\left(\mathbf{z}_i\right)\right)^2 = \frac{1}{n} \sum_{i=1}^n\left(\frac{y_i-\hat{f}\left(\mathbf{z}_i\right)}{1-\mathbf{S}_{i i}}\right)^2 
$$

Where $\hat{f}_{-i}\left(\mathbf{z}_i\right)$ is a statistical learning model $f$'s prediction without the $i$-th observation in the training set. $\mathbf{z}_i$ is the $i$-th observation, and $\mathbf{S}_{i i}$ is the $i$-th diagonal element of the hat matrix (a.k.a smoother, a matrix so that $\hat{y} = \mathbf{S} y$). **Keep in mind** that [time series cross-validation](https://thierrymoudiki.github.io/blog/2020/03/27/r/misc/crossval-2) will give different results. 

The LOOCV result can be extended further and approximated by **Generalized Cross-Validation (GCV)** (@golub1979generalized), still with a closed-form formula available. GCV is used in `ahead::ridge2f`. Indeed, even though `ridge2` is not -- strictly speaking -- linear, it possesses the structure of a [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) model with [2 regularization parameters](https://www.mdpi.com/2227-9091/6/1/22); a regularization parameter for the original explanative variables, and a regularization parameter for new, engineered features. These engineered features transform the linear model into a non-linear one.

In the following \textsf{R} example, it's worth mentioning that **only the 2 regularization parameters are calibrated**. Other model's hyperparameters such as the number of time series lags or the number of nodes in the hidden layer are set to their default values (respectively `1` and `5`). 


```{r}
objective_function <- function(xx)
{
    ahead::loocvridge2f(fpp::insurance,
                        h = 20,
                        type_pi="blockbootstrap",
                        lambda_1=10^xx[1],
                        lambda_2=10^xx[2],
                        show_progress = FALSE,
                        )$loocv
}
start <- proc.time()[3]
(opt <- dfoptim::nmkb(fn=objective_function, 
                      lower=c(-10,-10), 
                      upper=c(10,10), 
                      par=c(0.1, 0.1)))
print(proc.time()[3]-start)
```

\newpage

**Forecasting using the _optimal_ regularization parameters**

```{r}
start <- proc.time()[3]
res <- ahead::ridge2f(fpp::insurance, h = 20,
                      type_pi="blockbootstrap",
                      lambda_1=10^opt$par[1], # 'optimal' parameters
                      lambda_2=10^opt$par[2]) # 'optimal' parameters
print(proc.time()[3]-start)
```

```{r "3-figure-2-real-world"}
par(mfrow=c(2, 2))
plot(res, "Quotes", type = "sims",
main = "predictive simulations")
plot(res, "TV.advert", type = "sims",
main = "predictive simulations")
plot(res, "Quotes", type = "dist",
main = "prediction intervals")
plot(res, "TV.advert", type = "dist",
main = "prediction intervals")
```

# References
